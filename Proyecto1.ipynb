{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d15c3f4-e6da-4c52-8e25-33f536d6087c",
   "metadata": {},
   "source": [
    "# Proyecto 1\n",
    "* Flavio Galán - 22386\n",
    "* Josue Say - 22801\n",
    "* Isabella Miralles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4322479-0609-4733-9ce1-979917dbfc86",
   "metadata": {},
   "source": [
    "## Descarga de los Datos\n",
    "\n",
    "Para descargar los datos se utiliza Selenium y Python, además se tiene un sistema de cacheo que funciona en base al archivo `links_cache.txt`. Si este archivo ya existe entonces no se realizará ninguna descarga de los datos. Los datos se guardan en varios archivos txt. Cada uno representa la columna del dataframe y todos los datos de esa columna. No se tienen en un formato JSON ni CSV sino que se guardan directamente en formato python para la facilidad de extracción usando el mismo lenguaje. Ya que estos son los datos sucios no hay ningún problema con guardarlos así, los datos limpios ya se guardarán en un formaton más estandarizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83003afb",
   "metadata": {},
   "source": [
    "### Importaciones y configuración para el web scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "51ff481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import subprocess (se reemplazo con el uso de os para cualquier sistema operativo)\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b53dd8",
   "metadata": {},
   "source": [
    "### Configuración de rutas y archivo de caché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "106480f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file_name = \"./cache/links_cache.txt\"\n",
    "df_cache_file = \"./cache/dataframe_cache.txt\"\n",
    "zip_dir = \"zips/\"\n",
    "files_dir = \"data/\"\n",
    "csv_file_path = \"./data/data_unified.csv\"\n",
    "reports = \"reportes/\"\n",
    "\n",
    "\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"zips\", exist_ok=True)\n",
    "os.makedirs(\"reportes\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc4f2d",
   "metadata": {},
   "source": [
    "### Inicialización de estructuras para almacenar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "32b65acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo = []\n",
    "distrito = []\n",
    "departamento = []\n",
    "municipio = []\n",
    "establecimiento = []\n",
    "direccion = []\n",
    "telefono = []\n",
    "supervisor = []\n",
    "director = []\n",
    "nivel = []\n",
    "sector = []\n",
    "area = []\n",
    "status = []\n",
    "modalidad = []\n",
    "jornada = []\n",
    "plan = []\n",
    "departamental = []\n",
    "agggrArrays = [\n",
    "    codigo,\n",
    "    distrito,\n",
    "    departamento,\n",
    "    municipio,\n",
    "    establecimiento,\n",
    "    direccion,\n",
    "    telefono,\n",
    "    supervisor,\n",
    "    director,\n",
    "    nivel,\n",
    "    sector,\n",
    "    area,\n",
    "    status,\n",
    "    modalidad,\n",
    "    jornada,\n",
    "    plan,\n",
    "    departamental,\n",
    "]\n",
    "filenames = [\n",
    "    \"codigo\",\n",
    "    \"distrito\",\n",
    "    \"departamento\",\n",
    "    \"municipio\",\n",
    "    \"establecimiento\",\n",
    "    \"direccion\",\n",
    "    \"telefono\",\n",
    "    \"supervisor\",\n",
    "    \"director\",\n",
    "    \"nivel\",\n",
    "    \"sector\",\n",
    "    \"area\",\n",
    "    \"status\",\n",
    "    \"modalidad\",\n",
    "    \"jornada\",\n",
    "    \"plan\",\n",
    "    \"departamental\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09683fa0",
   "metadata": {},
   "source": [
    "### Carga de datos (web scrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "1e95a071-af81-4d6c-8a94-160f46a335bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(cache_file_name):\n",
    "    with webdriver.Firefox() as driver:\n",
    "        driver.implicitly_wait(3)\n",
    "\n",
    "        driver.get(\"http://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/\")\n",
    "        assert \"Búsqueda de centros\" in driver.title\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//*[@id='_ctl0_ContentPlaceHolder1_cmbDepartamento']\")\n",
    "            )\n",
    "        )\n",
    "        deptSelect = driver.find_element(\n",
    "            By.XPATH, \"//*[@id='_ctl0_ContentPlaceHolder1_cmbDepartamento']\"\n",
    "        )\n",
    "        deptSelect = Select(deptSelect)\n",
    "        depts = deptSelect.options\n",
    "        print(\"Getting depts\")\n",
    "        for idx in range(len(depts) - 1):\n",
    "            print(\"Selecting dept\", idx + 1)\n",
    "            deptSelect = driver.find_element(\n",
    "                By.XPATH, \"//*[@id='_ctl0_ContentPlaceHolder1_cmbDepartamento']\"\n",
    "            )\n",
    "            deptSelect = Select(deptSelect)\n",
    "            deptSelect.select_by_index(idx + 1)\n",
    "            print(\"Finding education level\")\n",
    "            levelSelect = driver.find_element(\n",
    "                By.XPATH, '//*[@id=\"_ctl0_ContentPlaceHolder1_cmbNivel\"]'\n",
    "            )\n",
    "            levelSelect = Select(levelSelect)\n",
    "            print(\"Selecting diversificado\")\n",
    "            levelSelect.select_by_value(\"46\")  # 46 es Diversificado\n",
    "            print(\"Finding search button\")\n",
    "            btn = driver.find_element(\n",
    "                By.XPATH, \"//*[@id='_ctl0_ContentPlaceHolder1_IbtnConsultar']\"\n",
    "            )\n",
    "            btn.click()\n",
    "            print(\"Clicking button\")\n",
    "\n",
    "            # Wait for results\n",
    "            print(\"Esperando por resultados\")\n",
    "            time.sleep(5)\n",
    "            print(\"Asumimos que se obtuvieron los resultados\")\n",
    "\n",
    "            table = driver.find_element(\n",
    "                By.XPATH, \"//*[@id='_ctl0_ContentPlaceHolder1_dgResultado']\"\n",
    "            )\n",
    "            rows = table.find_elements(By.XPATH, \".//tr\")\n",
    "            for rowIdx in range(\n",
    "                1, len(rows) - 1\n",
    "            ):  # La última fila siempre es una vacía\n",
    "                cells = rows[rowIdx].find_elements(By.XPATH, \".//td\")\n",
    "\n",
    "                for cellIdx in range(1, len(cells)):\n",
    "                    agggrArrays[cellIdx - 1].append(\n",
    "                        cells[cellIdx].get_attribute(\"textContent\")\n",
    "                    )\n",
    "\n",
    "            # print(\"Obtained following names\")\n",
    "            # print(establecimiento)\n",
    "            # exit(1)\n",
    "\n",
    "        print(\"Saving cache...\")\n",
    "        os.makedirs(zip_dir, exist_ok=True)\n",
    "        for idx in range(len(filenames)):\n",
    "            filename = filenames[idx]\n",
    "            data = agggrArrays[idx]\n",
    "\n",
    "            lines = [\"[\"]\n",
    "            for val in data:\n",
    "                line = f\"\\t'''{val}''',\\n\"\n",
    "                lines.append(line)\n",
    "            lines.append(\"]\")\n",
    "\n",
    "            with open(zip_dir + filename + \".txt\", \"w\") as file:\n",
    "                file.writelines(lines)\n",
    "\n",
    "        os.makedirs(os.path.dirname(cache_file_name), exist_ok=True)\n",
    "        with open(cache_file_name, \"w\") as file:\n",
    "            file.write(\"DELETE ME IF YOU WANT TO REDOWNLOAD DATA!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de999e",
   "metadata": {},
   "source": [
    "### Carga de datos al DataFrame\n",
    "\n",
    "Este bloque verifica si existen los archivos de caché (`links_cache.txt` y `dataframe_cache.txt`). Si no hay caché, construye el `DataFrame` desde los datos extraídos y guarda un `.csv`. Si los cachés ya existen, carga directamente el `DataFrame` desde el archivo CSV, evitando repetir procesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "a08c839c-a0e3-498e-9c62-457aed3d1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDfCache(\n",
    "    filenames=None,\n",
    "    agggrArrays=None,\n",
    "    csvFile=csv_file_path,\n",
    "    dfCache=df_cache_file\n",
    "):\n",
    "    if os.path.exists(csvFile):\n",
    "        df = pd.read_csv(csvFile, encoding=\"utf-8-sig\")\n",
    "        print(\"DataFrame loaded from CSV:\")\n",
    "        print(df)\n",
    "\n",
    "        if not os.path.exists(dfCache):\n",
    "            with open(dfCache, \"w\") as f:\n",
    "                f.write(\"DataFrame cache created.\")\n",
    "    else:\n",
    "        df = pd.DataFrame(\n",
    "            {colName: colData for (colName, colData) in zip(filenames, agggrArrays)}\n",
    "        )\n",
    "        print(\"The resulting DataFrame is:\")\n",
    "        print(df)\n",
    "\n",
    "        df.to_csv(csvFile, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        with open(dfCache, \"w\") as f:\n",
    "            f.write(\"DataFrame cache created.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "41ce6f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from CSV:\n",
      "             codigo distrito  departamento  municipio  \\\n",
      "0     16-01-0138-46   16-031  ALTA VERAPAZ      COBAN   \n",
      "1     16-01-0139-46   16-031  ALTA VERAPAZ      COBAN   \n",
      "2     16-01-0140-46   16-031  ALTA VERAPAZ      COBAN   \n",
      "3     16-01-0141-46   16-005  ALTA VERAPAZ      COBAN   \n",
      "4     16-01-0142-46   16-005  ALTA VERAPAZ      COBAN   \n",
      "...             ...      ...           ...        ...   \n",
      "6594  19-09-0040-46   19-021        ZACAPA   LA UNION   \n",
      "6595  19-09-0048-46   19-021        ZACAPA   LA UNION   \n",
      "6596  19-10-0013-46   19-015        ZACAPA      HUITE   \n",
      "6597  19-10-1009-46   19-015        ZACAPA      HUITE   \n",
      "6598  19-11-0018-46   19-020        ZACAPA  SAN JORGE   \n",
      "\n",
      "                                        establecimiento  \\\n",
      "0                                        COLEGIO  COBAN   \n",
      "1                     COLEGIO PARTICULAR MIXTO  VERAPAZ   \n",
      "2                               COLEGIO \"LA INMACULADA\"   \n",
      "3              ESCUELA NACIONAL DE CIENCIAS COMERCIALES   \n",
      "4     INSTITUTO NORMAL MIXTO DEL NORTE 'EMILIO ROSAL...   \n",
      "...                                                 ...   \n",
      "6594                     LICEO PARTICULAR MIXTO \"JIREH\"   \n",
      "6595                    LICEO PARTICULAR MIXTO  \"JIREH\"   \n",
      "6596                            INSTITUTO DIVERSIFICADO   \n",
      "6597            INSTITUTO DIVERSIFICADO POR COOPERATIVA   \n",
      "6598  INSTITUTO MIXTO DE EDUCACIÓN DIVERSIFICADA POR...   \n",
      "\n",
      "                                   direccion  telefono  \\\n",
      "0     KM.2 SALIDA A SAN JUAN CHAMELCO ZONA 8  77945104   \n",
      "1               KM 209.5 ENTRADA A LA CIUDAD  77367402   \n",
      "2                  7A. AVENIDA 11-109 ZONA 6  78232301   \n",
      "3                      2A CALLE 11-10 ZONA 2  79514215   \n",
      "4                        3A AVE 6-23 ZONA 11  79521468   \n",
      "...                                      ...       ...   \n",
      "6594                            BARRIO NUEVO  79418369   \n",
      "6595                            BARRIO NUEVO  79418369   \n",
      "6596                   BARRIO  BUENOS  AIRES  48579171   \n",
      "6597                        BARRIO EL  CAMPO  55958103   \n",
      "6598                        BARRIO EL CENTRO  41447589   \n",
      "\n",
      "                           supervisor                        director  \\\n",
      "0            PATRICIO NAJARRO ASENCIO       GUSTAVO ADOLFO SIERRA POP   \n",
      "1            PATRICIO NAJARRO ASENCIO  GILMA DOLORES GUAY PAZ DE LEAL   \n",
      "2            PATRICIO NAJARRO ASENCIO        VIRGINIA  SOLANO SERRANO   \n",
      "3     NORA LILIANA FIGUEROA HERNÁNDEZ        HÉCTOR ROLANDO CHUN POOU   \n",
      "4     NORA LILIANA FIGUEROA HERNÁNDEZ     VICTOR HUGO DOMÍNGUEZ REYES   \n",
      "...                               ...                             ...   \n",
      "6594        ASBEL IVÁN SÚCHITE ARROYO        ANA MARÍA CUELLAR GUERRA   \n",
      "6595        ASBEL IVÁN SÚCHITE ARROYO        ANA MARÍA CUELLAR GUERRA   \n",
      "6596       SILDY MARIELA PEREZ FRANCO       WUENDY JHOJANA SIERRA PAZ   \n",
      "6597       SILDY MARIELA PEREZ FRANCO      ROBIDIO  PORTILLO SALGUERO   \n",
      "6598                  ALBA LUZ MENDEZ       VICTOR HUGO GUERRA MONROY   \n",
      "\n",
      "              nivel       sector    area   status   modalidad      jornada  \\\n",
      "0     DIVERSIFICADO      PRIVADO  URBANA  ABIERTA  MONOLINGUE     MATUTINA   \n",
      "1     DIVERSIFICADO      PRIVADO  URBANA  ABIERTA  MONOLINGUE     MATUTINA   \n",
      "2     DIVERSIFICADO      PRIVADO  URBANA  ABIERTA  MONOLINGUE     MATUTINA   \n",
      "3     DIVERSIFICADO      OFICIAL  URBANA  ABIERTA  MONOLINGUE     MATUTINA   \n",
      "4     DIVERSIFICADO      OFICIAL  URBANA  ABIERTA    BILINGUE   VESPERTINA   \n",
      "...             ...          ...     ...      ...         ...          ...   \n",
      "6594  DIVERSIFICADO      PRIVADO  URBANA  ABIERTA  MONOLINGUE     MATUTINA   \n",
      "6595  DIVERSIFICADO      PRIVADO  URBANA  ABIERTA  MONOLINGUE  SIN JORNADA   \n",
      "6596  DIVERSIFICADO      OFICIAL  URBANA  ABIERTA  MONOLINGUE     NOCTURNA   \n",
      "6597  DIVERSIFICADO  COOPERATIVA  URBANA  ABIERTA  MONOLINGUE   VESPERTINA   \n",
      "6598  DIVERSIFICADO  COOPERATIVA  URBANA  ABIERTA  MONOLINGUE     MATUTINA   \n",
      "\n",
      "                                     plan departamental  \n",
      "0                         DIARIO(REGULAR)  ALTA VERAPAZ  \n",
      "1                         DIARIO(REGULAR)  ALTA VERAPAZ  \n",
      "2                         DIARIO(REGULAR)  ALTA VERAPAZ  \n",
      "3                         DIARIO(REGULAR)  ALTA VERAPAZ  \n",
      "4                         DIARIO(REGULAR)  ALTA VERAPAZ  \n",
      "...                                   ...           ...  \n",
      "6594                      DIARIO(REGULAR)        ZACAPA  \n",
      "6595  SEMIPRESENCIAL (UN DÍA A LA SEMANA)        ZACAPA  \n",
      "6596                      DIARIO(REGULAR)        ZACAPA  \n",
      "6597                      DIARIO(REGULAR)        ZACAPA  \n",
      "6598                      DIARIO(REGULAR)        ZACAPA  \n",
      "\n",
      "[6599 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "df = loadDfCache(filenames, agggrArrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24409521",
   "metadata": {},
   "source": [
    "## Estructura del Conjunto de Datos Crudo\n",
    "\n",
    "En esta etapa se realiza un análisis exploratorio preliminar del conjunto de datos descargado, con el objetivo de conocer su estructura general. Se identifica la cantidad de filas y columnas, la presencia de datos duplicados, valores nulos por variable, y los tipos de datos registrados junto con la primera limpieza para detectar los criterios mencionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "id": "227cbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataReport(df: pd.DataFrame, saveToFile: bool = True, filename: str = \"reporte_general\"):\n",
    "    \"\"\"\n",
    "    Genera un resumen general del DataFrame con limpieza de datos vacíos o invisibles.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame de entrada.\n",
    "    - saveToFile: Si es True, guarda el reporte en 'reportes/{filename}.txt'.\n",
    "    - filename: Nombre del archivo de texto (sin extensión).\n",
    "\n",
    "    Retorna:\n",
    "    - Tuple: (lista del reporte como strings, DataFrame limpio)\n",
    "    \"\"\"\n",
    "\n",
    "    # Limpieza profunda sin sobreescribir np.nan\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    def clean_value(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        val = str(val).replace(\"\\xa0\", \"\").strip()\n",
    "        return val if val != \"\" else np.nan\n",
    "\n",
    "    for col in df_clean.select_dtypes(include=[\"object\"]).columns:\n",
    "        df_clean[col] = df_clean[col].apply(clean_value)\n",
    "\n",
    "    if saveToFile:\n",
    "        os.makedirs(\"reportes\", exist_ok=True)\n",
    "        file_path = os.path.join(\"reportes\", f\"{filename}.txt\")\n",
    "\n",
    "    num_filas, num_columnas = df_clean.shape\n",
    "    duplicados_filas = df_clean.duplicated().sum()\n",
    "    nulos_por_columna = df_clean.isnull().sum()\n",
    "    tipos_datos = df_clean.dtypes\n",
    "\n",
    "    # Crear el reporte\n",
    "    reporte = []\n",
    "    reporte.append(f\"Total de filas: {num_filas}\")\n",
    "    reporte.append(f\"Total de columnas: {num_columnas}\")\n",
    "    reporte.append(f\"Filas duplicadas: {duplicados_filas}\")\n",
    "    reporte.append(\"\\nValores nulos por columna:\")\n",
    "    reporte.extend([f\"{col}: {nulos}\" for col, nulos in nulos_por_columna.items()])\n",
    "    reporte.append(\"\\nTipos de datos por columna:\")\n",
    "    reporte.extend([f\"{col}: {tipo}\" for col, tipo in tipos_datos.items()])\n",
    "\n",
    "    if saveToFile:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(reporte))\n",
    "        print(f\"Reporte generado en '{file_path}'\")\n",
    "\n",
    "    return reporte, df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "id": "58e2e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte generado en 'reportes\\reporte_data_v0.txt'\n"
     ]
    }
   ],
   "source": [
    "is_generate_report = True\n",
    "reporte, df_clean = generateDataReport(df, saveToFile=is_generate_report, filename=\"reporte_data_v0\")\n",
    "df_clean.to_csv(\"./data/data_clean_v0.csv\", index=False, encoding=\"utf-8\")\n",
    "# print(\"\\n\".join(report_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76210e",
   "metadata": {},
   "source": [
    "> Nota: Se puede modificar la bandera para omitir la creación del reporte inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d606e94-d7a3-473a-8627-a503dc52d91a",
   "metadata": {},
   "source": [
    "## Limpieza de los datos\n",
    "Se procede a ejecutar las transformaciones previamente ideadas y a unificar todos los datasets en uno solo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bffff71",
   "metadata": {},
   "source": [
    "### Importaciones y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "id": "55847b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ce7c9",
   "metadata": {},
   "source": [
    "### Establecimientos\n",
    "\n",
    "Primero normalizamos los textos eliminando comillas, tildes y espacios extra, dejando todo en mayúsculas. Luego, con base en reglas y excepciones definidas, identifica el tipo de establecimiento (como COLEGIO, INSTITUTO, ESCUELA, etc.) y lo guarda en una nueva columna. Finalmente, genera archivos con los establecimientos agrupados por tipo, listos para análisis o reporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "id": "7aefcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTextColumn(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Limpieza básica\n",
    "    df[column_name] = (\n",
    "        df[column_name]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[\\\"']\", \"\", regex=True)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.replace(r\",(?=\\s|$)\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Limpieza avanzada\n",
    "    def cleanText(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        text = re.sub(r\"[\\\"']\", \"\", text)\n",
    "        text = re.sub(r\",(?=\\s|$)\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text = unicodedata.normalize(\"NFKD\", text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        return text.strip()\n",
    "\n",
    "    df[column_name] = df[column_name].apply(cleanText)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "id": "df4f4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEstablishmentType(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    corrections = {\n",
    "        \"COLEGO\": \"COLEGIO\",\n",
    "        \"COLEIO\": \"COLEGIO\",\n",
    "        \"INSITUTO\": \"INSTITUTO\",\n",
    "        \"INSTITIUTO\": \"INSTITUTO\",\n",
    "        \"INTITUTO\": \"INSTITUTO\",\n",
    "        \"INSTITUTODE\": \"INSTITUTO\",\n",
    "        \"INST.\": \"INSTITUTO\",\n",
    "        \"'INSTITUTO\": \"INSTITUTO\",\n",
    "        \"'ESCUELA\": \"ESCUELA\",\n",
    "        \"'TECNOLOGICO\": \"TECNOLOGICO\"\n",
    "    }\n",
    "\n",
    "    def classifyAndFix(name: str) -> tuple[str, str]:\n",
    "        if pd.isna(name):\n",
    "            return name, None\n",
    "\n",
    "        name = name.strip().upper()\n",
    "        words = name.split()\n",
    "        if not words:\n",
    "            return name, None\n",
    "\n",
    "        first_word = words[0]\n",
    "        corrected_type = corrections.get(first_word, first_word)\n",
    "        words[0] = corrected_type\n",
    "        cleaned_name = \" \".join(words)\n",
    "\n",
    "        # Clasificación especial por contenido\n",
    "        if \"CEEX\" in cleaned_name:\n",
    "            corrected_type = \"CEEX\"\n",
    "        elif \"ISEA\" in cleaned_name:\n",
    "            corrected_type = \"HOMESCHOOL\"\n",
    "        elif \"LOVE GUATEMALA\" in cleaned_name:\n",
    "            corrected_type = \"CEEX\"\n",
    "        elif \"ZETH CENTRO EDUCATIVO\" in cleaned_name:\n",
    "            corrected_type = \"COLEGIO\"\n",
    "        elif \"IDEAS INSTITUTO\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "        elif cleaned_name.startswith(\"ISA \"):\n",
    "            corrected_type = \"ESCUELA\"\n",
    "        elif \"IMEBI\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "        elif \"GRUPO LA CEIBA\" in cleaned_name:\n",
    "            corrected_type = \"CEEX\"\n",
    "        elif cleaned_name == \"EL BOSQUE\":\n",
    "            corrected_type = \"ESCUELA\"\n",
    "        elif \"HAN AL INTERNACIONAL\" in cleaned_name:\n",
    "            corrected_type = \"COLEGIO\"\n",
    "        elif \"HOMESCHOOL\" in cleaned_name:\n",
    "            corrected_type = \"HOMESCHOOL\"\n",
    "        elif \"ENBI OXLAJUJ NO OJ\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "\n",
    "        # Excepciones: escuela pero debe ser instituto\n",
    "        elif \"INSTITUTO LA ESCUELA EN SU CASA\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "        elif \"INSTITUTO DE EDUCACION A DISTANCIA LA ESCUELA EN SU CASA\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "        elif \"INSTITUTO NORMAL DE PRIMARIA BILINGUE INTERCULTURAL ADS. A ESCUELA NORMAL\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "        elif \"INSTITUTO DE EDUCACION DIVERSIFICADA COLEGIO EUROPEO\" in cleaned_name:\n",
    "            corrected_type = \"INSTITUTO\"\n",
    "\n",
    "        # Si empieza con \"CORPORACION\"\n",
    "        elif cleaned_name.startswith(\"CORPORACION \"):\n",
    "            corrected_type = \"CORPORACION\"\n",
    "\n",
    "        elif corrected_type.startswith(\"ICA\"):\n",
    "            corrected_type = \"COLEGIO\"\n",
    "\n",
    "        # Clasificación general por palabra clave\n",
    "        elif \"COLEGIO\" in cleaned_name:\n",
    "            corrected_type = \"COLEGIO\"\n",
    "        elif \"ESCUELA\" in cleaned_name:\n",
    "            corrected_type = \"ESCUELA\"\n",
    "        elif \"SCHOOL\" in cleaned_name:\n",
    "            corrected_type = \"ESCUELA\"\n",
    "        elif \"COLLEGE\" in cleaned_name:\n",
    "            corrected_type = \"COLEGIO\"\n",
    "\n",
    "        # Mapeo general\n",
    "        if corrected_type == \"TECNOLOGICO\":\n",
    "            corrected_type = \"TECNICO\"\n",
    "\n",
    "        return cleaned_name, corrected_type\n",
    "\n",
    "\n",
    "    corrected_results = df[\"establecimiento\"].apply(classifyAndFix)\n",
    "    df[\"establecimiento\"] = corrected_results.apply(lambda x: x[0])\n",
    "    df[\"tipo_establecimiento\"] = corrected_results.apply(lambda x: x[1])\n",
    "\n",
    "    # Reordenar columna\n",
    "    cols = df.columns.tolist()\n",
    "    idx = cols.index(\"establecimiento\")\n",
    "    cols.insert(idx, cols.pop(cols.index(\"tipo_establecimiento\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "d2821bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleanTextColumn(df=df, column_name=\"establecimiento\")\n",
    "df.to_csv(\"./data/data_clean_v1.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "9e353af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporte, df_limpio = generateDataReport(df, saveToFile=is_generate_report, filename=\"reporte_data_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "2a94f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extractEstablishmentType(df)\n",
    "df.to_csv(\"./data/data_clean_v2.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "cffc7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporte, df_limpio = generateDataReport(df, saveToFile=is_generate_report, filename=\"reporte_data_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "1779e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueEstablishmentTypes(df: pd.DataFrame) -> list:\n",
    "    if \"tipo_establecimiento\" not in df.columns:\n",
    "        raise ValueError(\"Column 'tipo_establecimiento' not found in DataFrame.\")\n",
    "    \n",
    "    return df[\"tipo_establecimiento\"].dropna().unique().tolist()\n",
    "\n",
    "def saveTypesToFile(types: list, filepath: str):\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for tipo in types:\n",
    "            f.write(f\"- {tipo}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "abc859fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extractEstablishmentType(df)\n",
    "types = getUniqueEstablishmentTypes(df)\n",
    "saveTypesToFile(types, \"./reportes/tipos_establecimientos.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "d63b3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEstablishmentsByType(df: pd.DataFrame, tipo: str) -> None:\n",
    "    import os\n",
    "\n",
    "    tipo = tipo.strip().upper()\n",
    "    filtered_df = df[df[\"tipo_establecimiento\"] == tipo][[\"codigo\", \"establecimiento\"]]\n",
    "    os.makedirs(\"./reportes/detalle_establecimiento\", exist_ok=True)\n",
    "    file_name = f\"./reportes/detalle_establecimiento/establecimientos_{tipo.lower()}.csv\"\n",
    "    filtered_df.to_csv(file_name, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "eca8975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEstablishmentReports(df: pd.DataFrame, generateTxtFiles: bool = True) -> None:\n",
    "    if generateTxtFiles:\n",
    "        types = getUniqueEstablishmentTypes(df)\n",
    "        saveTypesToFile(types, \"./reportes/tipos_establecimientos.txt\")\n",
    "\n",
    "        for tipo in types:\n",
    "            saveEstablishmentsByType(df, tipo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "e730340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_generate_txt = True\n",
    "generateEstablishmentReports(df, is_generate_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e273d16",
   "metadata": {},
   "source": [
    "> Nota: Se puede modificar la bandera para omitir la creación de los reportes de establecimientos individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4454d",
   "metadata": {},
   "source": [
    "### Código\n",
    "\n",
    "Antes de procesar la columna `codigo`, se aplicó una limpieza previa a las columnas `departamento`, `municipio` y `departamental` para normalizar tildes, diéresis y caracteres especiales, garantizando uniformidad textual. Luego, se descompuso el valor de `codigo` siguiendo su estructura estándar (`XX-YY-ZZZZ-WW`), extrayendo tres componentes: el código de establecimiento (`ZZZZ`), el código interno (`WW`) y los códigos geográficos departamentales (`XX`) y municipales (`YY`). Estos se cruzaron con los nombres normalizados para generar una clave única `codigo_geografico`. Además, se extrajo el identificador del distrito (`YY` en `distrito`) como `codigo_distrito`, y se eliminaron las columnas redundantes: `codigo`, `departamento`, `municipio` y `distrito`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "7e9a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanGeographicNames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    def normalize(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        text = str(text).strip().upper()\n",
    "        text = unicodedata.normalize(\"NFKD\", text)\n",
    "        text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    for col in [\"departamento\", \"municipio\", \"departamental\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(normalize)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "0e998226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractGeographicCodes(df: pd.DataFrame, saveToCsv: bool = True) -> None:\n",
    "    df_codes = df[[\"codigo\", \"departamento\", \"municipio\"]].copy()\n",
    "\n",
    "    # Separar el código original\n",
    "    parts = df_codes[\"codigo\"].str.split(\"-\", expand=True)\n",
    "    df_codes[\"codigo_departamento\"] = parts[0]\n",
    "    df_codes[\"codigo_municipio\"] = parts[1]\n",
    "\n",
    "    # Obtener combinaciones únicas\n",
    "    df_geo = (\n",
    "        df_codes[[\"codigo_departamento\", \"departamento\", \"codigo_municipio\", \"municipio\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Agregar identificador único (secuencial)\n",
    "    df_geo.insert(0, \"codigo\", df_geo.index + 1)\n",
    "\n",
    "    if saveToCsv:\n",
    "        df_geo.to_csv(\"./data/codigos_geograficos.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "b2ba073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleanGeographicNames(df)\n",
    "is_generate_geo_codes = True\n",
    "extractGeographicCodes(df, is_generate_geo_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65229d",
   "metadata": {},
   "source": [
    "> Nota: Se puede modificar la bandera para omitir la creación del csv de códigos geográficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "e62b522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichWithCodes(df: pd.DataFrame, geoCodesPath: str = \"./data/codigos_geograficos.csv\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    parts = df[\"codigo\"].str.split(\"-\", expand=True)\n",
    "    df.insert(0, \"codigo_establecimiento\", parts[2])\n",
    "    df.insert(1, \"codigo_interno\", parts[3])\n",
    "    df[\"codigo_distrito\"] = df[\"distrito\"].str.split(\"-\").str[1]\n",
    "    df.drop(columns=[\"distrito\"], inplace=True)\n",
    "\n",
    "    # Reubicar código_distrito en la posición original de 'distrito' (posición 3)\n",
    "    cols = df.columns.tolist()\n",
    "    cod_dist = cols.pop(cols.index(\"codigo_distrito\"))\n",
    "    cols.insert(3, cod_dist)\n",
    "    df = df[cols]\n",
    "\n",
    "    df_geo = pd.read_csv(geoCodesPath).rename(columns={\"codigo\": \"codigo_geografico\"})\n",
    "\n",
    "    # Merge por departamento y municipio\n",
    "    df = df.merge(\n",
    "        df_geo[[\"codigo_geografico\", \"departamento\", \"municipio\"]],\n",
    "        how=\"left\",\n",
    "        on=[\"departamento\", \"municipio\"]\n",
    "    )\n",
    "\n",
    "    col = df.pop(\"codigo_geografico\")\n",
    "    df.insert(0, \"codigo_geografico\", col)\n",
    "\n",
    "    # Eliminar columnas ya integradas\n",
    "    df.drop(columns=[\"codigo\", \"departamento\", \"municipio\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "57cd302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = enrichWithCodes(df)\n",
    "df.to_csv(\"./data/data_clean_v3.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "c2d2308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporte, df_limpio = generateDataReport(df, saveToFile=is_generate_report, filename=\"reporte_data_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cbb9d",
   "metadata": {},
   "source": [
    "### Limpieza adicional para código geográfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "68962f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndMerge(data_df, codigos_df, is_unified):\n",
    "    if is_unified:\n",
    "        data_df['codigo_geografico'] = data_df['codigo_geografico'].astype(str)\n",
    "        codigos_df['codigo'] = codigos_df['codigo'].astype(str)\n",
    "        codigos_df['codigo_departamento'] = codigos_df['codigo_departamento'].astype(str)\n",
    "        codigos_df['codigo_municipio'] = codigos_df['codigo_municipio'].astype(str)\n",
    "\n",
    "        df_merged = data_df.merge(codigos_df, left_on='codigo_geografico', right_on='codigo', how='left')\n",
    "        df_merged.drop(columns=['codigo_geografico', 'codigo'], inplace=True)\n",
    "\n",
    "        columnas_geo = ['codigo_departamento', 'departamento', 'codigo_municipio', 'municipio']\n",
    "        otras_columnas = [col for col in df_merged.columns if col not in columnas_geo]\n",
    "        df_merged = df_merged[columnas_geo + otras_columnas]\n",
    "\n",
    "        # Forzar columnas clave como texto\n",
    "        for col in ['codigo_departamento', 'codigo_municipio', 'codigo_establecimiento', 'codigo_interno', 'codigo_distrito']:\n",
    "            if col in df_merged.columns:\n",
    "                df_merged[col] = df_merged[col].astype(str)\n",
    "\n",
    "        df_merged = df_merged.sort_values(by='codigo_departamento')\n",
    "        return df_merged\n",
    "    else:\n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "dd1a92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_unified = True\n",
    "data_v3 = pd.read_csv(\"./data/data_clean_v3.csv\", dtype={\n",
    "    \"codigo_geografico\": str,\n",
    "    \"codigo_establecimiento\": str,\n",
    "    \"codigo_interno\": str,\n",
    "    \"codigo_distrito\": str\n",
    "})\n",
    "\n",
    "codigos = pd.read_csv(\n",
    "    \"./data/codigos_geograficos.csv\",\n",
    "    dtype={\n",
    "        \"codigo\": str,\n",
    "        \"codigo_departamento\": str,\n",
    "        \"codigo_municipio\": str\n",
    "    }\n",
    ")\n",
    "\n",
    "data_final = cleanAndMerge(data_v3, codigos, is_unified)\n",
    "\n",
    "# Guardar preservando texto\n",
    "data_final.to_csv(\n",
    "    \"./data/data_clean_v4.csv\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_NONNUMERIC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "caafc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporte, df_limpio = generateDataReport(data_final, saveToFile=is_generate_report, filename=\"reporte_data_v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceef359",
   "metadata": {},
   "source": [
    ">Nota: para departamento y municipo se pueda saber su departamento por el codigo_geografico dado el csv \"codigos_geograficos.csv\" si no se hace la unificación con el código geográfico (is_unified=false)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40e286",
   "metadata": {},
   "source": [
    "### Departamento\n",
    "\n",
    "\n",
    "\n",
    "Este análisis se realizó con el objetivo de verificar si las columnas **\"departamento\"** y **\"departamental\"** contenían información redundante en el archivo `data_clean_v4.csv`. Inicialmente se asumía que ambas representaban lo mismo, y se evaluaba eliminar una por duplicidad.\n",
    "\n",
    "Sin embargo, al aplicar la función `checkDepartamentoDifference`, se identificaron múltiples casos donde los valores difieren. Esto reveló que:\n",
    "\n",
    "* **\"departamento\"** corresponde a la división político-administrativa oficial de Guatemala.\n",
    "* **\"departamental\"** representa subdivisiones organizacionales internas utilizadas para fines de gestión operativa, especialmente en el ámbito educativo.\n",
    "\n",
    "Como resultado, **no deben considerarse equivalentes ni eliminarse sin análisis**, ya que ambas aportan información distinta y útil para el contexto institucional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "1aeafebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkDepartamentoDifference(filePath):\n",
    "    # Cargar archivo\n",
    "    df = pd.read_csv(filePath)\n",
    "\n",
    "    # Filtrar donde 'departamento' y 'departamental' son distintos\n",
    "    diff_df = df[df['departamento'] != df['departamental']]\n",
    "\n",
    "    if not diff_df.empty:\n",
    "        # Agrupar por combinaciones distintas y guardar en CSV\n",
    "        grouped = diff_df[['departamento', 'departamental']].drop_duplicates()\n",
    "        grouped.to_csv('./data/difference_departamento_departamental.csv', index=False)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "3bbf32e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "result = checkDepartamentoDifference('./data/data_clean_v4.csv')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60422723",
   "metadata": {},
   "source": [
    "### Teléfono y categorías\n",
    "\n",
    "Se real\n",
    "izó una limpieza estructurada del dataset, que incluyó: normalización de nombres (`supervisor`, `director`) y campos tipográficos a mayúsculas, limpieza y estandarización de direcciones, validación de teléfonos (solo 8 dígitos), reemplazo de valores vacíos por `NaN` y conversión de columnas relevantes a tipo categórico (`status`, `sector`, `area`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "id": "c8695c97-2e51-4196-96f7-bbe866501762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Limpieza completada y archivo guardado como '/data/data_prev_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "df = data_final.copy()\n",
    "\n",
    "# Supervisor y director: limpieza + mayúsculas\n",
    "cols_normalizar = ['supervisor', 'director']\n",
    "for col in cols_normalizar:\n",
    "    df = cleanTextColumn(df, col)\n",
    "    df[col] = df[col].str.upper()\n",
    "\n",
    "# Tipográficas\n",
    "cols_tipograficas = ['modalidad', 'jornada', 'plan', 'status', 'sector', 'area']\n",
    "for col in cols_tipograficas:\n",
    "    df = cleanTextColumn(df, col)\n",
    "    df[col] = df[col].str.upper()\n",
    "\n",
    "# Vacíos como NaN\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Convertir columnas a categoría\n",
    "cols_categoricas = ['status', 'sector', 'area']\n",
    "for col in cols_categoricas:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Dirección: limpieza + normalización + mayúsculas\n",
    "df = cleanTextColumn(df, 'direccion')\n",
    "df['direccion'] = df['direccion'].str.upper()\n",
    "df['direccion'] = df['direccion'].str.replace(r'\\bKM\\b', 'KILÓMETRO', regex=True)\n",
    "df['direccion'] = df['direccion'].str.replace(r'\\bZONA\\b', 'ZONA', regex=True)\n",
    "\n",
    "# Teléfono: solo números válidos de 8 dígitos\n",
    "df['telefono'] = df['telefono'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "df['telefono'] = df['telefono'].apply(lambda x: x if len(x) == 8 else np.nan)\n",
    "\n",
    "# Guardar dataset limpio\n",
    "df.to_csv(\"./data/data_prev_clean.csv\", index=False)\n",
    "print(\"✅ Limpieza completada y archivo guardado como '/data/data_prev_clean.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5398ffd",
   "metadata": {},
   "source": [
    "## Generar CSV limpio\n",
    "\n",
    "Durante la limpieza final, se reemplazaron nulos con valores explícitos para mantener consistencia en el dataset:\n",
    "\n",
    "* `direccion` → `\"SIN DIRECCION\"`\n",
    "* `telefono` → `\"SIN TELEFONO\"`\n",
    "* `director` → `\"NO REGISTRADO\"`\n",
    "\n",
    "La persona encargada del análisis podrá decidir si desea imputar, eliminar o conservar estos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte generado en 'reportes\\reporte_data_prev_clean.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josue\\AppData\\Local\\Temp\\ipykernel_34856\\1800642574.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_limpio['direccion'].fillna(VALOR_NULO_DIRECCION, inplace=True)\n",
      "C:\\Users\\josue\\AppData\\Local\\Temp\\ipykernel_34856\\1800642574.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_limpio['telefono'].fillna(VALOR_NULO_TELEFONO, inplace=True)\n",
      "C:\\Users\\josue\\AppData\\Local\\Temp\\ipykernel_34856\\1800642574.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_limpio['director'].fillna(VALOR_NULO_DIRECTOR, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "reporte, df_limpio = generateDataReport(df, saveToFile=is_generate_report, filename=\"reporte_data_prev_clean\")\n",
    "\n",
    "VALOR_NULO_DIRECCION = \"SIN DIRECCION\"\n",
    "VALOR_NULO_TELEFONO = \"SIN TELEFONO\"\n",
    "VALOR_NULO_DIRECTOR = \"NO REGISTRADO\"\n",
    "df_limpio['direccion'].fillna(VALOR_NULO_DIRECCION, inplace=True)\n",
    "df_limpio['telefono'].fillna(VALOR_NULO_TELEFONO, inplace=True)\n",
    "df_limpio['director'].fillna(VALOR_NULO_DIRECTOR, inplace=True)\n",
    "\n",
    "df_limpio.to_csv(\"./data_clean.csv\", index=False, encoding=\"utf-8\")\n",
    "# print(\"\\n\".join(report_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "716c342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte generado en 'reportes\\reporte_data_clean.txt'\n"
     ]
    }
   ],
   "source": [
    "_, _ = generateDataReport(df_limpio, saveToFile=True, filename=\"reporte_data_clean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
